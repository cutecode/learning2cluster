NEXT STEPS:
- These line are very slow:
    def get_value(i):
        i_start = max(0, i - window_range)
        i_end = min(len(values), i + 1 + window_range)
        return sum(values[i_start:i_end]) / (i_end - i_start)
    res = list(map(get_value, range(len(values))))
  - Fix this! Use a running sum: sum the first (e.g. 10%) values together and the always subtract the first
    value and add the next value
- Print evaluation metrics to the command line (validation metrics)
  - Currently they are printed in the "Training" line, but obviously they should be printed in the validation line or just in another new line
- Use the weights for the try00
- Testing;)

TODO:
- Maybe print the network summary to somewhere?
- The validation loss seems to be always worse than the training loss, but the used data is always generated?
  - Test what happens if the training data is used for the validation run. See: BUG_LOSS.txt
- Im Konstruktor bei "super" jeweils als ersten Parameter die aktuelle Klasse mitgeben und nicht die Superklasse
- Optimizer state wird nicht richtig gespeichert / geladen, siehe: https://groups.google.com/forum/#!topic/keras-users/cNvLLrarfN4
- Prefix: "_" oder "__": entscheiden!
- Reihenfolge (private / protected / public) Zeugs mal aufräumen
- If weighted classes are used: Always print used weights compared with effective ratio
- Does the weighted loss work correct?
- Are metrics plotted for partially trained networks (see fixedc on the server: they are not plotted?)
- Weightec alsses improve the cluster detection accuracy, but sometimes multiple clusters are merged to one cluster.
  - Why? Because in the similarities output the "1"s create a low error, therefore for the network it is "ok" if there
    are too many "1"s. The clusters are always identified correctly, therefore the true "1s" are all detected.
  - Eventuell Bei Gewichtengen noch logarithmieren oder Wurzel oder so
- Add dropout to the try00 network

IDEAS:
- Try00 nehmen mit mehr LSTM-Layer und mehr Dense-Units
- Try00 mit wsich wiederholenden LSTM-Layern
- Try00 mit nur einer möglichen Cluster-Anzahl testen
  - Der DataProvider darf nun auf keinen Fall mehr zu wenige Cluster liefern
- Try00 in der Ausgabe mit nur einer Cluster-Anzahl testen, effektiv können aber beliebig viel Cluster in der Eingabe sein, aber
  weniger oder gleich viele wie im neuronalen Netz möglich sind. Die effektive Anzahl Cluster wird festgelegt durch die
  nicht leeren Cluster in der Ausgabe

NICE2HAVE:
- Email-Notify einbauen
- Geiler: Ein Telegram Bot: Mit Kommandos wie "example" können Beispiele erstellt und angefragt werden

DONE:
- Save optimizer config (to_config(), from_config())
  - save/load optimizer state
- Implement save/load network model
  - Abstract in BaseNN, impl in subclasses
- Implement save/load training history
  - This could be included in the general save load method (add a flag "include_history" or something like this)
- Implement something to always store the best model etc.
  - Maybe implement events or something like this
- Implement validation
  - Use test data for this
- Implement some nice plots (at least validation / training loss)
- Validate only every nth iteration
- Create a nice output while training (something like lasagne / nolearn) -> the current output sucks
  - Use the own history object to generate a nice output
- Always print best iteration
- Cluster Count Distribution wird nicht richtig geplottet
- Print and store the required time for each iteration
- Always print loss plots
- Iteration print: Only print non-NaN values
- Saving files:
  - First create a temporary file
  - After writing it, move it to the target location
  - Why? Because if someone stops the program it is very uncool to have broken weights etc.
- Print total training time
- The prediction pictures are wrong
- Create colorized output during the training
  - Also: Order the output by training, then validation
  - There is an empty line between the time and the training line
  - Highlight the iteration number (and the best loss)
  - If a loss is the best loss, print is green
  - For the iteration output: Also print the best training loss
- Print the amount of network parameters
- Prediction graphics for all cluster counts: Add the probability to the title
- Implement evaluation metrics
  - For testing and also for the history (only while validating)
  - Evaluation metrics plots
- Cluster plots for 2D data: Make the points a bit transparent (becaue of overlapping points)
- Implement an empirical weight measurement
  - Sample data until the relative expected count of 0s and 1s has a confidence interval of less then 1%
  - Implement a nice print (see TODO)
- Git-Repo erstellen
- Use weithed loss-values for the similarities output
    - Calculate the expected 1s and 0s and weight them
    - See: https://github.com/fchollet/keras/issues/2115
           https://github.com/fchollet/keras/issues/3068
           https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras
           https://github.com/fchollet/keras/issues/4735
    - See: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy
        - Maybe use the log / sqrt of the weights?
        - Implement log / sqrt / f(x) normalize => use the current weights and appliy w_0n = f(w_0), and sum(f(w_i)) = count of weights
    - Test the current implementation somehow
        - Create a minimal network and do some tests: A network without weights and a defined and fixed input, check
          if the loss is exactly equal to the manually computed value. Another test is to directly check the output
          of the weighted and unweighted binary crossentropys
    - For the weighted cross entropy: The total weight sum should be the number of all summands and not 1 like now. (obviously)
    - For debugging: Print the actual percentage of zeros and ones in the y values
    - Use a samples mean: Just sample data until the 95% interval of the mean is smaller than XYZ
        - See: https://stackoverflow.com/a/15034143/916672, or better: https://stackoverflow.com/a/34474255/916672
- Use the wrapper "concat_layer" for concatenations. Why? The "Concatenate" layer cannot handle single inputs
- Cluster counts things are buggy (see fixedc, and see scr 4)
- fixedc contains a Bug (see BUG_FIXEC): A bug while generating data. This has to be fixed
- Bei Graphen eine Art Average-Kurve durchziehen
- DataProvider aufräumen: Man soll einen Range doer eine fixe ANzahl CLuster mitgeben. Aktuell ist das ein wenig Bastel
- Implement debug-Outputs: Just print them in some nice format (e.g. use a Prefix DEBUG_ or something like this, or add
  a bool "debug" to the _s_layer method)
  - Edit: While building the network one may add layers to an additional array "debug_output"
- Falls kein Cluster COunt beim Output ist, ist der cluster count graph leer, aber trotzdem vorhanden
  - Entweder Grpah entfernen oder einen knmstanten Wert von 1.0 einfügen
- Scores implementieren: http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation
  - Program evaluate_metrics in cluster_nn.py
